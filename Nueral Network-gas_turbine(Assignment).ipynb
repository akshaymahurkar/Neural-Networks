{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6ff0ab",
   "metadata": {},
   "source": [
    "# Gas_turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1a85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b4271f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "gas = pd.read_csv(\"D:/ExcelR/Assignments/Downloaded/Neural networks/gas_turbines.csv\")\n",
    "gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb946ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15039 entries, 0 to 15038\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      15039 non-null  float64\n",
      " 1   AP      15039 non-null  float64\n",
      " 2   AH      15039 non-null  float64\n",
      " 3   AFDP    15039 non-null  float64\n",
      " 4   GTEP    15039 non-null  float64\n",
      " 5   TIT     15039 non-null  float64\n",
      " 6   TAT     15039 non-null  float64\n",
      " 7   TEY     15039 non-null  float64\n",
      " 8   CDP     15039 non-null  float64\n",
      " 9   CO      15039 non-null  float64\n",
      " 10  NOX     15039 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "gas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2084d2b",
   "metadata": {},
   "source": [
    "-No null values from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad624534",
   "metadata": {},
   "source": [
    "#### correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41766cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>-0.549432</td>\n",
       "      <td>-0.099333</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.093067</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.207495</td>\n",
       "      <td>-0.100705</td>\n",
       "      <td>-0.088588</td>\n",
       "      <td>-0.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>-0.412953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.256744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AH</th>\n",
       "      <td>-0.549432</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>-0.247781</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>0.143061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFDP</th>\n",
       "      <td>-0.099333</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>0.727152</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.037299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEP</th>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874526</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>0.993784</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIT</th>\n",
       "      <td>0.093067</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.247781</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>0.874526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.357320</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>-0.688272</td>\n",
       "      <td>-0.231636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAT</th>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>-0.357320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>-0.744740</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEY</th>\n",
       "      <td>-0.207495</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988473</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>-0.102631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDP</th>\n",
       "      <td>-0.100705</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>0.727152</td>\n",
       "      <td>0.993784</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>-0.744740</td>\n",
       "      <td>0.988473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.520783</td>\n",
       "      <td>-0.169103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>-0.088588</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.688272</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>-0.520783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.600006</td>\n",
       "      <td>0.256744</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>-0.037299</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.231636</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.102631</td>\n",
       "      <td>-0.169103</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AT        AP        AH      AFDP      GTEP       TIT       TAT  \\\n",
       "AT    1.000000 -0.412953 -0.549432 -0.099333 -0.049103  0.093067  0.338569   \n",
       "AP   -0.412953  1.000000  0.042573  0.040318  0.078575  0.029650 -0.223479   \n",
       "AH   -0.549432  0.042573  1.000000 -0.119249 -0.202784 -0.247781  0.010859   \n",
       "AFDP -0.099333  0.040318 -0.119249  1.000000  0.744251  0.627254 -0.571541   \n",
       "GTEP -0.049103  0.078575 -0.202784  0.744251  1.000000  0.874526 -0.756884   \n",
       "TIT   0.093067  0.029650 -0.247781  0.627254  0.874526  1.000000 -0.357320   \n",
       "TAT   0.338569 -0.223479  0.010859 -0.571541 -0.756884 -0.357320  1.000000   \n",
       "TEY  -0.207495  0.146939 -0.110272  0.717995  0.977042  0.891587 -0.720356   \n",
       "CDP  -0.100705  0.131198 -0.182010  0.727152  0.993784  0.887238 -0.744740   \n",
       "CO   -0.088588  0.041614  0.165505 -0.334207 -0.508259 -0.688272  0.063404   \n",
       "NOX  -0.600006  0.256744  0.143061 -0.037299 -0.208496 -0.231636  0.009888   \n",
       "\n",
       "           TEY       CDP        CO       NOX  \n",
       "AT   -0.207495 -0.100705 -0.088588 -0.600006  \n",
       "AP    0.146939  0.131198  0.041614  0.256744  \n",
       "AH   -0.110272 -0.182010  0.165505  0.143061  \n",
       "AFDP  0.717995  0.727152 -0.334207 -0.037299  \n",
       "GTEP  0.977042  0.993784 -0.508259 -0.208496  \n",
       "TIT   0.891587  0.887238 -0.688272 -0.231636  \n",
       "TAT  -0.720356 -0.744740  0.063404  0.009888  \n",
       "TEY   1.000000  0.988473 -0.541751 -0.102631  \n",
       "CDP   0.988473  1.000000 -0.520783 -0.169103  \n",
       "CO   -0.541751 -0.520783  1.000000  0.316743  \n",
       "NOX  -0.102631 -0.169103  0.316743  1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ede5f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of duplicated rows\n",
    "gas[gas.duplicated()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d44efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550.01    270\n",
       "550.00    268\n",
       "550.04    266\n",
       "550.03    253\n",
       "549.98    252\n",
       "         ... \n",
       "536.25      1\n",
       "539.04      1\n",
       "534.51      1\n",
       "529.28      1\n",
       "525.37      1\n",
       "Name: TAT, Length: 2340, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas['TAT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87457c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating VIF for multicolinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d326e3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT</td>\n",
       "      <td>3.414439e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP</td>\n",
       "      <td>1.842147e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AH</td>\n",
       "      <td>6.863745e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFDP</td>\n",
       "      <td>8.580114e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GTEP</td>\n",
       "      <td>1.233878e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TIT</td>\n",
       "      <td>1.372092e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TAT</td>\n",
       "      <td>8.166382e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CDP</td>\n",
       "      <td>3.858125e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CO</td>\n",
       "      <td>3.813210e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NOX</td>\n",
       "      <td>1.071006e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variables           VIF\n",
       "0        AT  3.414439e+01\n",
       "1        AP  1.842147e+04\n",
       "2        AH  6.863745e+01\n",
       "3      AFDP  8.580114e+01\n",
       "4      GTEP  1.233878e+04\n",
       "5       TIT  1.372092e+06\n",
       "6       TAT  8.166382e+05\n",
       "7       CDP  3.858125e+04\n",
       "8        CO  3.813210e+00\n",
       "9       NOX  1.071006e+02"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = gas.drop('TEY',axis=1)\n",
    "calc_vif(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb82780",
   "metadata": {},
   "source": [
    "###### Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5b7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_func(i):\n",
    "     x = (i-i.min())/(i.max()-i.min())\n",
    "     return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31eacdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting The Data\n",
    "predictors = gas.drop('TEY',axis=1)\n",
    "target = gas['TEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26003170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15039"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67bcceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15039"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3fc430b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15039, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.shape #shape of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "127c2aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15039,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape  #shape of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cd43586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.184182</td>\n",
       "      <td>0.456050</td>\n",
       "      <td>0.951314</td>\n",
       "      <td>0.255758</td>\n",
       "      <td>0.091426</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.984015</td>\n",
       "      <td>0.135340</td>\n",
       "      <td>0.071522</td>\n",
       "      <td>0.596548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.182020</td>\n",
       "      <td>0.466391</td>\n",
       "      <td>0.955881</td>\n",
       "      <td>0.255721</td>\n",
       "      <td>0.094755</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.984015</td>\n",
       "      <td>0.133988</td>\n",
       "      <td>0.073372</td>\n",
       "      <td>0.597134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.185295</td>\n",
       "      <td>0.474664</td>\n",
       "      <td>0.939003</td>\n",
       "      <td>0.252571</td>\n",
       "      <td>0.097367</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.980608</td>\n",
       "      <td>0.134567</td>\n",
       "      <td>0.072576</td>\n",
       "      <td>0.593791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.189922</td>\n",
       "      <td>0.482937</td>\n",
       "      <td>0.929126</td>\n",
       "      <td>0.252227</td>\n",
       "      <td>0.098033</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.983753</td>\n",
       "      <td>0.135533</td>\n",
       "      <td>0.072375</td>\n",
       "      <td>0.595984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.199830</td>\n",
       "      <td>0.493278</td>\n",
       "      <td>0.927708</td>\n",
       "      <td>0.255323</td>\n",
       "      <td>0.096650</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.983491</td>\n",
       "      <td>0.136692</td>\n",
       "      <td>0.073647</td>\n",
       "      <td>0.592087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>0.247272</td>\n",
       "      <td>0.408480</td>\n",
       "      <td>0.975092</td>\n",
       "      <td>0.263380</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.884696</td>\n",
       "      <td>0.095739</td>\n",
       "      <td>0.102448</td>\n",
       "      <td>0.562214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.414685</td>\n",
       "      <td>0.984153</td>\n",
       "      <td>0.256826</td>\n",
       "      <td>0.078672</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.806342</td>\n",
       "      <td>0.102113</td>\n",
       "      <td>0.109894</td>\n",
       "      <td>0.566100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>0.195962</td>\n",
       "      <td>0.422958</td>\n",
       "      <td>0.989922</td>\n",
       "      <td>0.251593</td>\n",
       "      <td>0.084614</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.651730</td>\n",
       "      <td>0.111772</td>\n",
       "      <td>0.180552</td>\n",
       "      <td>0.685449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>0.188443</td>\n",
       "      <td>0.433299</td>\n",
       "      <td>0.982936</td>\n",
       "      <td>0.246451</td>\n",
       "      <td>0.076777</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.754455</td>\n",
       "      <td>0.121431</td>\n",
       "      <td>0.141693</td>\n",
       "      <td>0.710578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>0.186173</td>\n",
       "      <td>0.441572</td>\n",
       "      <td>0.961821</td>\n",
       "      <td>0.242631</td>\n",
       "      <td>0.073141</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.875262</td>\n",
       "      <td>0.131090</td>\n",
       "      <td>0.112946</td>\n",
       "      <td>0.702665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             AT        AP        AH      AFDP      GTEP    TIT       TAT  \\\n",
       "0      0.184182  0.456050  0.951314  0.255758  0.091426  0.584  0.984015   \n",
       "1      0.182020  0.466391  0.955881  0.255721  0.094755  0.585  0.984015   \n",
       "2      0.185295  0.474664  0.939003  0.252571  0.097367  0.586  0.980608   \n",
       "3      0.189922  0.482937  0.929126  0.252227  0.098033  0.588  0.983753   \n",
       "4      0.199830  0.493278  0.927708  0.255323  0.096650  0.589  0.983491   \n",
       "...         ...       ...       ...       ...       ...    ...       ...   \n",
       "15034  0.247272  0.408480  0.975092  0.263380  0.065868  0.489  0.884696   \n",
       "15035  0.214075  0.414685  0.984153  0.256826  0.078672  0.455  0.806342   \n",
       "15036  0.195962  0.422958  0.989922  0.251593  0.084614  0.369  0.651730   \n",
       "15037  0.188443  0.433299  0.982936  0.246451  0.076777  0.424  0.754455   \n",
       "15038  0.186173  0.441572  0.961821  0.242631  0.073141  0.491  0.875262   \n",
       "\n",
       "            CDP        CO       NOX  \n",
       "0      0.135340  0.071522  0.596548  \n",
       "1      0.133988  0.073372  0.597134  \n",
       "2      0.134567  0.072576  0.593791  \n",
       "3      0.135533  0.072375  0.595984  \n",
       "4      0.136692  0.073647  0.592087  \n",
       "...         ...       ...       ...  \n",
       "15034  0.095739  0.102448  0.562214  \n",
       "15035  0.102113  0.109894  0.566100  \n",
       "15036  0.111772  0.180552  0.685449  \n",
       "15037  0.121431  0.141693  0.710578  \n",
       "15038  0.131090  0.112946  0.702665  \n",
       "\n",
       "[15039 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding normalization to train data.\n",
    "predictors1 = norm_func(predictors)\n",
    "predictors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85b1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using train_test_split splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test= train_test_split(predictors1,target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c181ac5",
   "metadata": {},
   "source": [
    "## building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb53ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(12, activation='relu', input_shape=(10,)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "790f2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08146378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "878/878 [==============================] - 10s 1ms/step - loss: -2009.2361 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "878/878 [==============================] - 1s 612us/step - loss: -2032.9597 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "878/878 [==============================] - 1s 673us/step - loss: -2029.5856 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "878/878 [==============================] - 1s 652us/step - loss: -2028.9347 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "878/878 [==============================] - 1s 654us/step - loss: -2031.9228 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "878/878 [==============================] - 1s 669us/step - loss: -2030.1242 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "878/878 [==============================] - 1s 636us/step - loss: -2027.1884 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "878/878 [==============================] - 1s 616us/step - loss: -2032.6798 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "878/878 [==============================] - 1s 644us/step - loss: -2030.6119 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "878/878 [==============================] - 1s 656us/step - loss: -2030.1142 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "878/878 [==============================] - 1s 678us/step - loss: -2033.9401 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "878/878 [==============================] - 1s 648us/step - loss: -2029.4099 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "878/878 [==============================] - 1s 650us/step - loss: -2029.8837 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "878/878 [==============================] - 1s 607us/step - loss: -2030.5686 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "878/878 [==============================] - 1s 606us/step - loss: -2030.7894 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "878/878 [==============================] - 1s 645us/step - loss: -2033.3671 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "878/878 [==============================] - 1s 646us/step - loss: -2031.0233 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "878/878 [==============================] - 1s 657us/step - loss: -2030.1081 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "878/878 [==============================] - 1s 669us/step - loss: -2030.7382 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "878/878 [==============================] - 1s 652us/step - loss: -2032.2918 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "878/878 [==============================] - 1s 625us/step - loss: -2028.1917 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "878/878 [==============================] - 1s 674us/step - loss: -2033.0712 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "878/878 [==============================] - 1s 691us/step - loss: -2032.2541 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "878/878 [==============================] - 1s 661us/step - loss: -2030.6852 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "878/878 [==============================] - 1s 657us/step - loss: -2032.1665 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "878/878 [==============================] - 1s 654us/step - loss: -2027.2246 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "878/878 [==============================] - 1s 632us/step - loss: -2032.6977 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "878/878 [==============================] - 1s 644us/step - loss: -2026.9656 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "878/878 [==============================] - 1s 721us/step - loss: -2032.3291 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "878/878 [==============================] - 1s 736us/step - loss: -2031.6248 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "878/878 [==============================] - 1s 674us/step - loss: -2033.7156 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "878/878 [==============================] - 1s 658us/step - loss: -2030.1317 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "878/878 [==============================] - 1s 633us/step - loss: -2027.3379 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "878/878 [==============================] - 1s 595us/step - loss: -2037.1151 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "878/878 [==============================] - 1s 597us/step - loss: -2030.6708 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "878/878 [==============================] - 1s 646us/step - loss: -2032.2382 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "878/878 [==============================] - 1s 647us/step - loss: -2034.8716 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "878/878 [==============================] - 1s 645us/step - loss: -2031.0390 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "878/878 [==============================] - 1s 647us/step - loss: -2033.6022 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "878/878 [==============================] - 1s 627us/step - loss: -2029.2188 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "878/878 [==============================] - 1s 607us/step - loss: -2030.9676 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "878/878 [==============================] - 1s 645us/step - loss: -2030.5836 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "878/878 [==============================] - 1s 677us/step - loss: -2033.2530 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "878/878 [==============================] - 1s 666us/step - loss: -2028.7960 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "878/878 [==============================] - 1s 670us/step - loss: -2028.7691 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "878/878 [==============================] - 1s 653us/step - loss: -2033.6030 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "878/878 [==============================] - 1s 623us/step - loss: -2033.2846 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "878/878 [==============================] - 1s 614us/step - loss: -2025.0028 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "878/878 [==============================] - 1s 662us/step - loss: -2029.8816 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "878/878 [==============================] - 1s 666us/step - loss: -2029.5339 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "878/878 [==============================] - 1s 666us/step - loss: -2034.4245 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "878/878 [==============================] - 1s 648us/step - loss: -2032.6532 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "878/878 [==============================] - 1s 649us/step - loss: -2031.3887 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "878/878 [==============================] - 1s 590us/step - loss: -2027.6666 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "878/878 [==============================] - 1s 630us/step - loss: -2030.1435 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "878/878 [==============================] - 1s 652us/step - loss: -2031.4472 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "878/878 [==============================] - 1s 660us/step - loss: -2028.9612 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "878/878 [==============================] - 1s 665us/step - loss: -2034.8946 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "878/878 [==============================] - 1s 640us/step - loss: -2032.0127 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "878/878 [==============================] - 1s 612us/step - loss: -2029.4318 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "878/878 [==============================] - 1s 587us/step - loss: -2032.7047 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "878/878 [==============================] - 1s 605us/step - loss: -2031.5440 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "878/878 [==============================] - 1s 644us/step - loss: -2031.2428 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "878/878 [==============================] - 1s 639us/step - loss: -2032.0395 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "878/878 [==============================] - 1s 637us/step - loss: -2034.6679 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "878/878 [==============================] - 1s 641us/step - loss: -2028.3973 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "878/878 [==============================] - 1s 623us/step - loss: -2032.3435 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "878/878 [==============================] - 1s 580us/step - loss: -2028.1778 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "878/878 [==============================] - 1s 612us/step - loss: -2031.0172 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "878/878 [==============================] - 1s 638us/step - loss: -2034.2948 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "878/878 [==============================] - 1s 640us/step - loss: -2026.9716 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "878/878 [==============================] - 1s 650us/step - loss: -2031.0807 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "878/878 [==============================] - 1s 645us/step - loss: -2029.1134 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "878/878 [==============================] - 1s 602us/step - loss: -2030.4425 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "878/878 [==============================] - 1s 606us/step - loss: -2030.9896 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "878/878 [==============================] - 1s 668us/step - loss: -2035.3987 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "878/878 [==============================] - 1s 657us/step - loss: -2026.9914 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "878/878 [==============================] - 1s 652us/step - loss: -2032.6136 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "878/878 [==============================] - 1s 654us/step - loss: -2031.5812 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "878/878 [==============================] - 1s 643us/step - loss: -2034.0170 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "878/878 [==============================] - 1s 602us/step - loss: -2032.8838 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "878/878 [==============================] - 1s 614us/step - loss: -2029.5732 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "878/878 [==============================] - 1s 650us/step - loss: -2030.1660 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "878/878 [==============================] - 1s 646us/step - loss: -2030.9380 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "878/878 [==============================] - 1s 648us/step - loss: -2030.7712 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "878/878 [==============================] - 1s 645us/step - loss: -2031.0895 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "878/878 [==============================] - 1s 622us/step - loss: -2029.5887 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "878/878 [==============================] - 1s 594us/step - loss: -2028.7942 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "878/878 [==============================] - 1s 624us/step - loss: -2033.7457 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "878/878 [==============================] - 1s 652us/step - loss: -2036.1897 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "878/878 [==============================] - 1s 650us/step - loss: -2032.3938 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "878/878 [==============================] - 1s 645us/step - loss: -2032.1337 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "878/878 [==============================] - 1s 646us/step - loss: -2033.9519 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "878/878 [==============================] - 1s 604us/step - loss: -2027.7047 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "878/878 [==============================] - 1s 612us/step - loss: -2027.5016 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "878/878 [==============================] - 1s 657us/step - loss: -2029.7322 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "878/878 [==============================] - 1s 649us/step - loss: -2030.6081 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "878/878 [==============================] - 1s 654us/step - loss: -2032.8516 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "878/878 [==============================] - 1s 660us/step - loss: -2032.9005 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "878/878 [==============================] - 1s 641us/step - loss: -2033.5149 - accuracy: 0.0000e+00 - val_loss: -2031.2948 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(np.array(x_train),np.array(y_train),\n",
    "          batch_size=12, epochs=100,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7e95e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 0s 591us/step - loss: -2031.2946 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7bb69",
   "metadata": {},
   "source": [
    "As we are getting 0.0 accuracy we will build a diff model using different loss function and different optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d630c4b",
   "metadata": {},
   "source": [
    "## building another new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6924a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(10,)),\n",
    "    Dense(12, activation='relu'),\n",
    "    Dense(1, activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efc8aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5774e5f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "878/878 [==============================] - 1s 815us/step - loss: 11736.0220 - mse: 11736.0220 - val_loss: 90.3979 - val_mse: 90.3979\n",
      "Epoch 2/100\n",
      "878/878 [==============================] - 1s 633us/step - loss: 81.5486 - mse: 81.5486 - val_loss: 51.9001 - val_mse: 51.9001\n",
      "Epoch 3/100\n",
      "878/878 [==============================] - 1s 639us/step - loss: 46.4619 - mse: 46.4619 - val_loss: 32.0359 - val_mse: 32.0359\n",
      "Epoch 4/100\n",
      "878/878 [==============================] - 1s 631us/step - loss: 30.6926 - mse: 30.6926 - val_loss: 26.5509 - val_mse: 26.5509\n",
      "Epoch 5/100\n",
      "878/878 [==============================] - 1s 623us/step - loss: 26.6792 - mse: 26.6792 - val_loss: 24.0897 - val_mse: 24.0897\n",
      "Epoch 6/100\n",
      "878/878 [==============================] - 1s 595us/step - loss: 23.4418 - mse: 23.4418 - val_loss: 21.0398 - val_mse: 21.0398\n",
      "Epoch 7/100\n",
      "878/878 [==============================] - 1s 621us/step - loss: 20.7336 - mse: 20.7336 - val_loss: 18.1170 - val_mse: 18.1170\n",
      "Epoch 8/100\n",
      "878/878 [==============================] - 1s 625us/step - loss: 16.8131 - mse: 16.8131 - val_loss: 14.7128 - val_mse: 14.7128\n",
      "Epoch 9/100\n",
      "878/878 [==============================] - 1s 631us/step - loss: 13.9488 - mse: 13.9488 - val_loss: 11.4787 - val_mse: 11.4787\n",
      "Epoch 10/100\n",
      "878/878 [==============================] - 1s 644us/step - loss: 11.0448 - mse: 11.0448 - val_loss: 9.4312 - val_mse: 9.4312\n",
      "Epoch 11/100\n",
      "878/878 [==============================] - 1s 630us/step - loss: 7.9588 - mse: 7.9588 - val_loss: 5.4557 - val_mse: 5.4557\n",
      "Epoch 12/100\n",
      "878/878 [==============================] - 1s 613us/step - loss: 5.0692 - mse: 5.0692 - val_loss: 3.2865 - val_mse: 3.2865\n",
      "Epoch 13/100\n",
      "878/878 [==============================] - 1s 594us/step - loss: 3.0550 - mse: 3.0550 - val_loss: 2.1512 - val_mse: 2.1512\n",
      "Epoch 14/100\n",
      "878/878 [==============================] - 1s 620us/step - loss: 2.1634 - mse: 2.1634 - val_loss: 1.7711 - val_mse: 1.7711\n",
      "Epoch 15/100\n",
      "878/878 [==============================] - 1s 633us/step - loss: 1.7176 - mse: 1.7176 - val_loss: 1.4221 - val_mse: 1.4221\n",
      "Epoch 16/100\n",
      "878/878 [==============================] - 1s 630us/step - loss: 1.4597 - mse: 1.4597 - val_loss: 1.2252 - val_mse: 1.2252\n",
      "Epoch 17/100\n",
      "878/878 [==============================] - 1s 628us/step - loss: 1.2972 - mse: 1.2972 - val_loss: 1.3813 - val_mse: 1.3813\n",
      "Epoch 18/100\n",
      "878/878 [==============================] - 1s 631us/step - loss: 1.1574 - mse: 1.1574 - val_loss: 1.1890 - val_mse: 1.1890\n",
      "Epoch 19/100\n",
      "878/878 [==============================] - 1s 655us/step - loss: 1.0831 - mse: 1.0831 - val_loss: 1.0061 - val_mse: 1.0061\n",
      "Epoch 20/100\n",
      "878/878 [==============================] - 1s 600us/step - loss: 0.9796 - mse: 0.9796 - val_loss: 0.8766 - val_mse: 0.8766\n",
      "Epoch 21/100\n",
      "878/878 [==============================] - 1s 637us/step - loss: 0.9179 - mse: 0.9179 - val_loss: 0.8278 - val_mse: 0.8278\n",
      "Epoch 22/100\n",
      "878/878 [==============================] - 1s 652us/step - loss: 0.8543 - mse: 0.8543 - val_loss: 0.7904 - val_mse: 0.7904\n",
      "Epoch 23/100\n",
      "878/878 [==============================] - 1s 655us/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 24/100\n",
      "878/878 [==============================] - 1s 672us/step - loss: 0.8469 - mse: 0.8469 - val_loss: 0.7505 - val_mse: 0.7505\n",
      "Epoch 25/100\n",
      "878/878 [==============================] - 1s 649us/step - loss: 0.8122 - mse: 0.8122 - val_loss: 0.8980 - val_mse: 0.8980\n",
      "Epoch 26/100\n",
      "878/878 [==============================] - 1s 613us/step - loss: 0.7940 - mse: 0.7940 - val_loss: 0.7228 - val_mse: 0.7228\n",
      "Epoch 27/100\n",
      "878/878 [==============================] - 1s 602us/step - loss: 0.7362 - mse: 0.7362 - val_loss: 0.8617 - val_mse: 0.8617\n",
      "Epoch 28/100\n",
      "878/878 [==============================] - 1s 637us/step - loss: 0.7664 - mse: 0.7664 - val_loss: 0.9413 - val_mse: 0.9413\n",
      "Epoch 29/100\n",
      "878/878 [==============================] - 1s 644us/step - loss: 0.7431 - mse: 0.7431 - val_loss: 0.7650 - val_mse: 0.7650\n",
      "Epoch 30/100\n",
      "878/878 [==============================] - 1s 673us/step - loss: 0.7161 - mse: 0.7161 - val_loss: 0.7027 - val_mse: 0.7027\n",
      "Epoch 31/100\n",
      "878/878 [==============================] - 1s 639us/step - loss: 0.7225 - mse: 0.7225 - val_loss: 0.7840 - val_mse: 0.7840\n",
      "Epoch 32/100\n",
      "878/878 [==============================] - 1s 633us/step - loss: 0.7194 - mse: 0.7194 - val_loss: 0.7527 - val_mse: 0.7527\n",
      "Epoch 33/100\n",
      "878/878 [==============================] - 1s 605us/step - loss: 0.7041 - mse: 0.7041 - val_loss: 0.8887 - val_mse: 0.8887\n",
      "Epoch 34/100\n",
      "878/878 [==============================] - 1s 591us/step - loss: 0.7088 - mse: 0.7088 - val_loss: 0.7640 - val_mse: 0.7640\n",
      "Epoch 35/100\n",
      "878/878 [==============================] - 1s 635us/step - loss: 0.6645 - mse: 0.6645 - val_loss: 0.6720 - val_mse: 0.6720\n",
      "Epoch 36/100\n",
      "878/878 [==============================] - 1s 633us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7032 - val_mse: 0.7032\n",
      "Epoch 37/100\n",
      "878/878 [==============================] - 1s 633us/step - loss: 0.7499 - mse: 0.7499 - val_loss: 0.7276 - val_mse: 0.7276\n",
      "Epoch 38/100\n",
      "878/878 [==============================] - 1s 633us/step - loss: 0.6735 - mse: 0.6735 - val_loss: 0.7467 - val_mse: 0.7467\n",
      "Epoch 39/100\n",
      "878/878 [==============================] - 1s 629us/step - loss: 0.6690 - mse: 0.6690 - val_loss: 0.6819 - val_mse: 0.6819\n",
      "Epoch 40/100\n",
      "878/878 [==============================] - 1s 621us/step - loss: 0.6901 - mse: 0.6901 - val_loss: 0.6475 - val_mse: 0.6475\n",
      "Epoch 41/100\n",
      "878/878 [==============================] - 1s 653us/step - loss: 0.6758 - mse: 0.6758 - val_loss: 0.9282 - val_mse: 0.9282\n",
      "Epoch 42/100\n",
      "878/878 [==============================] - 1s 660us/step - loss: 0.6812 - mse: 0.6812 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 43/100\n",
      "878/878 [==============================] - 1s 671us/step - loss: 0.6685 - mse: 0.6685 - val_loss: 0.7833 - val_mse: 0.7833\n",
      "Epoch 44/100\n",
      "878/878 [==============================] - 1s 653us/step - loss: 0.6693 - mse: 0.6693 - val_loss: 0.6379 - val_mse: 0.6379\n",
      "Epoch 45/100\n",
      "878/878 [==============================] - 1s 641us/step - loss: 0.6322 - mse: 0.6322 - val_loss: 0.6515 - val_mse: 0.6515\n",
      "Epoch 46/100\n",
      "878/878 [==============================] - 1s 638us/step - loss: 0.6459 - mse: 0.6459 - val_loss: 0.6694 - val_mse: 0.6694\n",
      "Epoch 47/100\n",
      "878/878 [==============================] - 1s 608us/step - loss: 0.6600 - mse: 0.6600 - val_loss: 0.6350 - val_mse: 0.6350\n",
      "Epoch 48/100\n",
      "878/878 [==============================] - 1s 608us/step - loss: 0.6405 - mse: 0.6405 - val_loss: 0.6279 - val_mse: 0.6279\n",
      "Epoch 49/100\n",
      "878/878 [==============================] - 1s 636us/step - loss: 0.6528 - mse: 0.6528 - val_loss: 1.1309 - val_mse: 1.1309\n",
      "Epoch 50/100\n",
      "878/878 [==============================] - 1s 647us/step - loss: 0.6691 - mse: 0.6691 - val_loss: 0.6730 - val_mse: 0.6730\n",
      "Epoch 51/100\n",
      "878/878 [==============================] - 1s 643us/step - loss: 0.6293 - mse: 0.6293 - val_loss: 0.6513 - val_mse: 0.6513\n",
      "Epoch 52/100\n",
      "878/878 [==============================] - 1s 638us/step - loss: 0.6167 - mse: 0.6167 - val_loss: 0.7134 - val_mse: 0.7134\n",
      "Epoch 53/100\n",
      "878/878 [==============================] - 1s 635us/step - loss: 0.6460 - mse: 0.6460 - val_loss: 0.6314 - val_mse: 0.6314\n",
      "Epoch 54/100\n",
      "878/878 [==============================] - 1s 588us/step - loss: 0.6016 - mse: 0.6016 - val_loss: 0.5990 - val_mse: 0.5990\n",
      "Epoch 55/100\n",
      "878/878 [==============================] - 1s 599us/step - loss: 0.6294 - mse: 0.6294 - val_loss: 0.7504 - val_mse: 0.7504\n",
      "Epoch 56/100\n",
      "878/878 [==============================] - 1s 627us/step - loss: 0.6131 - mse: 0.6131 - val_loss: 0.6954 - val_mse: 0.6954\n",
      "Epoch 57/100\n",
      "878/878 [==============================] - 1s 639us/step - loss: 0.6029 - mse: 0.6029 - val_loss: 0.5975 - val_mse: 0.5975\n",
      "Epoch 58/100\n",
      "878/878 [==============================] - 1s 632us/step - loss: 0.5902 - mse: 0.5902 - val_loss: 0.6214 - val_mse: 0.6214\n",
      "Epoch 59/100\n",
      "878/878 [==============================] - 1s 630us/step - loss: 0.6526 - mse: 0.6526 - val_loss: 0.5902 - val_mse: 0.5902\n",
      "Epoch 60/100\n",
      "878/878 [==============================] - 1s 611us/step - loss: 0.6060 - mse: 0.6060 - val_loss: 0.5955 - val_mse: 0.5955\n",
      "Epoch 61/100\n",
      "878/878 [==============================] - 1s 594us/step - loss: 0.6067 - mse: 0.6067 - val_loss: 0.5896 - val_mse: 0.5896\n",
      "Epoch 62/100\n",
      "878/878 [==============================] - 1s 604us/step - loss: 0.5912 - mse: 0.5912 - val_loss: 0.5969 - val_mse: 0.5969\n",
      "Epoch 63/100\n",
      "878/878 [==============================] - 1s 624us/step - loss: 0.6156 - mse: 0.6156 - val_loss: 0.5926 - val_mse: 0.5926\n",
      "Epoch 64/100\n",
      "878/878 [==============================] - 1s 629us/step - loss: 0.5999 - mse: 0.5999 - val_loss: 0.7354 - val_mse: 0.7354\n",
      "Epoch 65/100\n",
      "878/878 [==============================] - 1s 615us/step - loss: 0.5811 - mse: 0.5811 - val_loss: 0.6116 - val_mse: 0.6116\n",
      "Epoch 66/100\n",
      "878/878 [==============================] - 1s 615us/step - loss: 0.6213 - mse: 0.6213 - val_loss: 0.6218 - val_mse: 0.6218\n",
      "Epoch 67/100\n",
      "878/878 [==============================] - 1s 602us/step - loss: 0.5812 - mse: 0.5812 - val_loss: 0.6729 - val_mse: 0.6729\n",
      "Epoch 68/100\n",
      "878/878 [==============================] - 1s 588us/step - loss: 0.5994 - mse: 0.5994 - val_loss: 0.5599 - val_mse: 0.5599\n",
      "Epoch 69/100\n",
      "878/878 [==============================] - 1s 611us/step - loss: 0.5601 - mse: 0.5601 - val_loss: 0.5655 - val_mse: 0.5655\n",
      "Epoch 70/100\n",
      "878/878 [==============================] - 1s 629us/step - loss: 0.5657 - mse: 0.5657 - val_loss: 0.6351 - val_mse: 0.6351\n",
      "Epoch 71/100\n",
      "878/878 [==============================] - 1s 622us/step - loss: 0.5902 - mse: 0.5902 - val_loss: 0.5678 - val_mse: 0.5678\n",
      "Epoch 72/100\n",
      "878/878 [==============================] - 1s 622us/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.5938 - val_mse: 0.5938\n",
      "Epoch 73/100\n",
      "878/878 [==============================] - 1s 625us/step - loss: 0.5609 - mse: 0.5609 - val_loss: 0.5554 - val_mse: 0.5554\n",
      "Epoch 74/100\n",
      "878/878 [==============================] - 1s 591us/step - loss: 0.5673 - mse: 0.5673 - val_loss: 0.5474 - val_mse: 0.5474\n",
      "Epoch 75/100\n",
      "878/878 [==============================] - 1s 591us/step - loss: 0.5464 - mse: 0.5464 - val_loss: 0.6893 - val_mse: 0.6893\n",
      "Epoch 76/100\n",
      "878/878 [==============================] - 1s 613us/step - loss: 0.5804 - mse: 0.5804 - val_loss: 0.6125 - val_mse: 0.6125\n",
      "Epoch 77/100\n",
      "878/878 [==============================] - 1s 622us/step - loss: 0.5795 - mse: 0.5795 - val_loss: 0.5441 - val_mse: 0.5441\n",
      "Epoch 78/100\n",
      "878/878 [==============================] - 1s 620us/step - loss: 0.5620 - mse: 0.5620 - val_loss: 0.5509 - val_mse: 0.5509\n",
      "Epoch 79/100\n",
      "878/878 [==============================] - 1s 622us/step - loss: 0.5655 - mse: 0.5655 - val_loss: 0.5668 - val_mse: 0.5668\n",
      "Epoch 80/100\n",
      "878/878 [==============================] - 1s 621us/step - loss: 0.5492 - mse: 0.5492 - val_loss: 0.5476 - val_mse: 0.5476\n",
      "Epoch 81/100\n",
      "878/878 [==============================] - 1s 591us/step - loss: 0.5603 - mse: 0.5603 - val_loss: 0.5927 - val_mse: 0.5927\n",
      "Epoch 82/100\n",
      "878/878 [==============================] - 1s 583us/step - loss: 0.5838 - mse: 0.5838 - val_loss: 0.6510 - val_mse: 0.6510\n",
      "Epoch 83/100\n",
      "878/878 [==============================] - 1s 615us/step - loss: 0.5524 - mse: 0.5524 - val_loss: 1.2067 - val_mse: 1.2067\n",
      "Epoch 84/100\n",
      "878/878 [==============================] - 1s 616us/step - loss: 0.5656 - mse: 0.5656 - val_loss: 0.6853 - val_mse: 0.6853\n",
      "Epoch 85/100\n",
      "878/878 [==============================] - 1s 625us/step - loss: 0.5509 - mse: 0.5509 - val_loss: 0.5352 - val_mse: 0.5352\n",
      "Epoch 86/100\n",
      "878/878 [==============================] - 1s 621us/step - loss: 0.5731 - mse: 0.5731 - val_loss: 0.5923 - val_mse: 0.5923\n",
      "Epoch 87/100\n",
      "878/878 [==============================] - 1s 632us/step - loss: 0.5504 - mse: 0.5504 - val_loss: 0.5332 - val_mse: 0.5332\n",
      "Epoch 88/100\n",
      "878/878 [==============================] - 1s 653us/step - loss: 0.5635 - mse: 0.5635 - val_loss: 0.5534 - val_mse: 0.5534\n",
      "Epoch 89/100\n",
      "878/878 [==============================] - 1s 633us/step - loss: 0.5809 - mse: 0.5809 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "Epoch 90/100\n",
      "878/878 [==============================] - 1s 623us/step - loss: 0.5625 - mse: 0.5625 - val_loss: 0.5225 - val_mse: 0.5225\n",
      "Epoch 91/100\n",
      "878/878 [==============================] - 1s 635us/step - loss: 0.5230 - mse: 0.5230 - val_loss: 0.5899 - val_mse: 0.5899\n",
      "Epoch 92/100\n",
      "878/878 [==============================] - 1s 652us/step - loss: 0.5494 - mse: 0.5494 - val_loss: 0.5550 - val_mse: 0.5550\n",
      "Epoch 93/100\n",
      "878/878 [==============================] - 1s 613us/step - loss: 0.5424 - mse: 0.5424 - val_loss: 0.5243 - val_mse: 0.5243\n",
      "Epoch 94/100\n",
      "878/878 [==============================] - 1s 695us/step - loss: 0.5453 - mse: 0.5453 - val_loss: 0.6216 - val_mse: 0.6216\n",
      "Epoch 95/100\n",
      "878/878 [==============================] - 1s 612us/step - loss: 0.5381 - mse: 0.5381 - val_loss: 0.5227 - val_mse: 0.5227\n",
      "Epoch 96/100\n",
      "878/878 [==============================] - 1s 595us/step - loss: 0.5780 - mse: 0.5780 - val_loss: 0.6001 - val_mse: 0.6001\n",
      "Epoch 97/100\n",
      "878/878 [==============================] - 1s 615us/step - loss: 0.5231 - mse: 0.5231 - val_loss: 0.5274 - val_mse: 0.5274\n",
      "Epoch 98/100\n",
      "878/878 [==============================] - 1s 625us/step - loss: 0.5203 - mse: 0.5203 - val_loss: 0.5824 - val_mse: 0.5824\n",
      "Epoch 99/100\n",
      "878/878 [==============================] - 1s 647us/step - loss: 0.5286 - mse: 0.5286 - val_loss: 0.6957 - val_mse: 0.6957\n",
      "Epoch 100/100\n",
      "878/878 [==============================] - 1s 631us/step - loss: 0.5505 - mse: 0.5505 - val_loss: 0.8609 - val_mse: 0.8609\n"
     ]
    }
   ],
   "source": [
    "hist_1 = model1.fit(np.array(x_train),np.array(y_train),\n",
    "          batch_size=12, epochs=100,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "078a874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 0s 634us/step - loss: 0.8609 - mse: 0.8609\n",
      "mse: 86.09%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model1.evaluate(x_test, y_test)\n",
    "print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d73a2b",
   "metadata": {},
   "source": [
    "Here we got some better results \n",
    "Therefore we choose a model with loss funtion = \"mean_squared_error\",optimizer = \"adam\" .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c5cb22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualize the train data\n",
    "# list all data in history\n",
    "model1.history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69f45212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist_1.history['mse'])\n",
    "plt.plot(hist_1.history['val_mse'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63515fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAf/ElEQVR4nO3dfZTcVZ3n8fenqjrpPPKQpw1pNJnZqDwoAWIMAzOLohBACYpgVJRVdqIe9oh7BCXOqoeZzS5nZpZFHEFRGMIiYAQZMko0EMGHBQkdjAIBTBQkTWISA4EkJJ2k890/fre7qqurujpJVyrp/rzOqVO/uvV7uLeTrk/f372/XykiMDMz602u0RUwM7ODn8PCzMxqcliYmVlNDgszM6vJYWFmZjU5LMzMrCaHhVk/k3SrpP/Rx3VfkPTu/d2PWb05LMzMrCaHhZmZ1eSwsEEpnf65UtJvJW2TdLOkCZIWS9oi6UFJR5Ssf56kpyVtlvSwpGNK3jtR0hNpu+8BzWXHeq+kFWnbRyS9bR/r/LeSVkt6WdIiSUelckn6P5I2SHo1ten49N45klamur0k6Yp9+oHZoOewsMHsAuA9wJuA9wGLgS8BY8l+Nz4LIOlNwJ3A54BxwP3Av0saImkI8G/A/wWOBL6f9kva9iTgFuBTwBjgW8AiSUP3pqKS3gX8L+AiYCLwR+Cu9PaZwN+kdhwOfAjYlN67GfhURIwCjgd+ujfHNevksLDB7OsRsT4iXgJ+ATwWEb+OiHbgXuDEtN6HgB9FxAMRsQv4Z2AY8FfATKAJuC4idkXE3cDjJcf4W+BbEfFYRHRExAKgPW23Nz4K3BIRT6T6zQNOkTQZ2AWMAt4CKCKeiYh1abtdwLGSRkfEKxHxxF4e1wxwWNjgtr5keXuF1yPT8lFkf8kDEBF7gDXApPTeS9H9jpx/LFl+I/D5dApqs6TNwNFpu71RXoetZL2HSRHxU+BfgG8A6yXdJGl0WvUC4Bzgj5J+JumUvTyuGeCwMOuLtWQf+kA2RkD2gf8SsA6YlMo6vaFkeQ0wPyIOL3kMj4g797MOI8hOa70EEBHXR8TJwHFkp6OuTOWPR8RsYDzZ6bKFe3lcM8BhYdYXC4FzJZ0hqQn4PNmppEeAR4HdwGclFSR9AJhRsu23gU9LekcaiB4h6VxJo/ayDncAn5A0LY13/E+y02YvSHp72n8TsA3YAXSkMZWPSjosnT57DejYj5+DDWIOC7MaIuI54GLg68CfyQbD3xcROyNiJ/AB4D8Dr5CNb/ygZNtWsnGLf0nvr07r7m0dlgJfBu4h6838JTAnvT2aLJReITtVtYlsXAXgY8ALkl4DPp3aYbbX5C8/MjOzWtyzMDOzmhwWZmZWk8PCzMxqcliYmVlNhUZXoF7Gjh0bkydPbnQ1zMwOKcuXL/9zRIwrLx+wYTF58mRaW1sbXQ0zs0OKpD9WKvdpKDMzq8lhYWZmNTkszMyspgE7ZmFmtrd27dpFW1sbO3bsaHRV6q65uZmWlhaampr6tL7DwswsaWtrY9SoUUyePJnuNxIeWCKCTZs20dbWxpQpU/q0jU9DmZklO3bsYMyYMQM6KAAkMWbMmL3qQTkszMxKDPSg6LS37XRYlLn1/z3Pot+sbXQ1zMwOKg6LMncse5HFT66rvaKZWT/bvHkzN9xww15vd84557B58+b+r1AJh0WZfC7H7j3+jg8zO/CqhUVHR+9fcHj//fdz+OGH16lWGc+GKlPIiQ6HhZk1wFVXXcXvf/97pk2bRlNTEyNHjmTixImsWLGClStXcv7557NmzRp27NjB5Zdfzty5c4Hi7Y22bt3K2WefzWmnncYjjzzCpEmTuO+++xg2bNh+181hUSafk3sWZsbV//40K9e+1q/7PPao0Xz1fcdVff+aa67hqaeeYsWKFTz88MOce+65PPXUU13TW2+55RaOPPJItm/fztvf/nYuuOACxowZ020fq1at4s477+Tb3/42F110Effccw8XX7z/36brsCiT9Sz2NLoaZmbMmDGj23UQ119/Pffeey8Aa9asYdWqVT3CYsqUKUybNg2Ak08+mRdeeKFf6uKwKJPPid0d7lmYDXa99QAOlBEjRnQtP/zwwzz44IM8+uijDB8+nNNPP73idRJDhw7tWs7n82zfvr1f6uIB7jKFvMcszKwxRo0axZYtWyq+9+qrr3LEEUcwfPhwnn32WX71q18d0Lq5Z1Emmw3V+8wDM7N6GDNmDKeeeirHH388w4YNY8KECV3vzZo1i29+85u87W1v481vfjMzZ848oHVzWJTxbCgza6Q77rijYvnQoUNZvHhxxfc6xyXGjh3LU0891VV+xRVX9Fu9fBqqjGdDmZn15LAo49lQZmY9OSzKuGdhZtaTw6KMxyzMzHpyWJTJ53K+zsLMrIzDoox7FmZmPTksyuTzHrMws8bY11uUA1x33XW8/vrr/VyjIodFGc+GMrNGOZjDwhfllfFsKDNrlNJblL/nPe9h/PjxLFy4kPb2dt7//vdz9dVXs23bNi666CLa2tro6Ojgy1/+MuvXr2ft2rW8853vZOzYsTz00EP9XjeHRRmPWZgZAIuvgj892b/7/A9vhbOvqfp26S3KlyxZwt13382yZcuICM477zx+/vOfs3HjRo466ih+9KMfAdk9ow477DCuvfZaHnroIcaOHdu/dU58GqqMvynPzA4GS5YsYcmSJZx44omcdNJJPPvss6xatYq3vvWtPPjgg3zxi1/kF7/4BYcddtgBqY97FmXcszAzoNcewIEQEcybN49PfepTPd5bvnw5999/P/PmzePMM8/kK1/5St3r455FmXwKiwgHhpkdWKW3KD/rrLO45ZZb2Lp1KwAvvfQSGzZsYO3atQwfPpyLL76YK664gieeeKLHtvVQ156FpBeALUAHsDsipks6EvgeMBl4AbgoIl5J688DLk3rfzYifpLKTwZuBYYB9wOXR50+zQs5AdCxJyjkVY9DmJlVVHqL8rPPPpuPfOQjnHLKKQCMHDmS22+/ndWrV3PllVeSy+VoamrixhtvBGDu3LmcffbZTJw4sS4D3KrnX9ApLKZHxJ9Lyv4ReDkirpF0FXBERHxR0rHAncAM4CjgQeBNEdEhaRlwOfArsrC4PiIq36s3mT59erS2tu51nW94eDX/+OPnePYfZtHclN/r7c3s0PXMM89wzDHHNLoaB0yl9kpaHhHTy9dtxGmo2cCCtLwAOL+k/K6IaI+I54HVwAxJE4HREfFo6k3cVrJNvyvtWZiZWabeYRHAEknLJc1NZRMiYh1Aeh6fyicBa0q2bUtlk9JyeXkPkuZKapXUunHjxn2qcD6X/Ug8I8rMrKjes6FOjYi1ksYDD0h6tpd1Kw0QRC/lPQsjbgJuguw01N5WFtyzMBvsIgJp4I9X7u0QRF17FhGxNj1vAO4lG49Yn04tkZ43pNXbgKNLNm8B1qbylgrldZFPYbG7w7f8MBtsmpub2bRp04CfDRkRbNq0iebm5j5vU7eehaQRQC4itqTlM4G/BxYBlwDXpOf70iaLgDskXUs2wD0VWJYGuLdImgk8Bnwc+Hq96t3Zs/BpKLPBp6Wlhba2Nvb1NPahpLm5mZaWltorJvU8DTUBuDd15wrAHRHxY0mPAwslXQq8CFwIEBFPS1oIrAR2A5dFREfa12coTp1dnB51kfdpKLNBq6mpiSlTpjS6GgeluoVFRPwBOKFC+SbgjCrbzAfmVyhvBY7v7zpW0nlthXsWZmZFvoK7TCHNhvJtys3MihwWZTxmYWbWk8OiTHE2lMPCzKyTw6JM55iFB7jNzIocFmV8BbeZWU8OizK+gtvMrCeHRZmuMQvPhjIz6+KwKOOehZlZTw6LMnlPnTUz68FhUabrojxPnTUz6+KwKOOehZlZTw6LMr7OwsysJ4dFGc+GMjPryWFRxrOhzMx6cliU8ZiFmVlPDosyxVuUOyzMzDo5LMq4Z2Fm1pPDokzXmEWHB7jNzDo5LMrk/bWqZmY9OCzKeDaUmVlPDosyHrMwM+vJYVHGs6HMzHpyWJRJHQv3LMzMSjgsykiikBMdvt2HmVkXh0UF+ZzcszAzK1H3sJCUl/RrST9Mr4+U9ICkVen5iJJ150laLek5SWeVlJ8s6cn03vWSVM86F3Ly91mYmZU4ED2Ly4FnSl5fBSyNiKnA0vQaSccCc4DjgFnADZLyaZsbgbnA1PSYVc8Ku2dhZtZdXcNCUgtwLvCdkuLZwIK0vAA4v6T8rohoj4jngdXADEkTgdER8WhEBHBbyTZ1UcjnfItyM7MS9e5ZXAd8ASj95J0QEesA0vP4VD4JWFOyXlsqm5SWy8t7kDRXUquk1o0bN+5zpfM5eeqsmVmJuoWFpPcCGyJieV83qVAWvZT3LIy4KSKmR8T0cePG9fGwPRVyYrfHLMzMuhTquO9TgfMknQM0A6Ml3Q6slzQxItalU0wb0vptwNEl27cAa1N5S4XyunHPwsysu7r1LCJiXkS0RMRksoHrn0bExcAi4JK02iXAfWl5ETBH0lBJU8gGspelU1VbJM1Ms6A+XrJNXTTlcx7gNjMrUc+eRTXXAAslXQq8CFwIEBFPS1oIrAR2A5dFREfa5jPArcAwYHF61I17FmZm3R2QsIiIh4GH0/Im4Iwq680H5lcobwWOr18Nuyvk5NlQZmYlfAV3Be5ZmJl157CooOCL8szMunFYVOCehZlZdw6LCgq5nK+zMDMr4bCowD0LM7PuHBYVFPKeDWVmVsphUYF7FmZm3TksKvBsKDOz7hwWFbhnYWbWncOigkLO94YyMyvlsKjAPQszs+4cFhX43lBmZt05LCrI50SHL8ozM+visKggu87CYWFm1slhUYHHLMzMunNYVODZUGZm3TksKnDPwsysO4dFBZ4NZWbWncOiAvcszMy6c1hU4HtDmZl157CoIJ/LEQF7HBhmZoDDoqJCXgDuXZiZJQ6LCvK5LCw8bmFmlnFYVFBIYbHLM6LMzACHRUVdPQvfH8rMDHBYVNTZs/CYhZlZpm5hIalZ0jJJv5H0tKSrU/mRkh6QtCo9H1GyzTxJqyU9J+mskvKTJT2Z3rtekupVb8hmQ4HHLMzMOtWzZ9EOvCsiTgCmAbMkzQSuApZGxFRgaXqNpGOBOcBxwCzgBkn5tK8bgbnA1PSYVcd6l/QsPGZhZgZ9DAtJl0sarczNkp6QdGZv20Rma3rZlB4BzAYWpPIFwPlpeTZwV0S0R8TzwGpghqSJwOiIeDQiAritZJu66Jw6656FmVmmrz2LT0bEa8CZwDjgE8A1tTaSlJe0AtgAPBARjwETImIdQHoen1afBKwp2bwtlU1Ky+XllY43V1KrpNaNGzf2sWk95T1mYWbWTV/DonOM4BzgXyPiNyVlVUVER0RMA1rIegnH9+EY3XbRS3ml490UEdMjYvq4ceNqVa+qgscszMy66WtYLJe0hCwsfiJpFNDnE/oRsRl4mGysYX06tUR63pBWawOOLtmsBVibylsqlNdNV8/CU2fNzIC+h8WlZAPRb4+I18nGHz7R2waSxkk6PC0PA94NPAssAi5Jq10C3JeWFwFzJA2VNIVsIHtZOlW1RdLMNAvq4yXb1EXBV3CbmXVT6ON6pwArImKbpIuBk4Cv1dhmIrAgzWjKAQsj4oeSHgUWSroUeBG4ECAinpa0EFgJ7AYui4iOtK/PALcCw4DF6VE3+bxnQ5mZleprWNwInCDpBOALwM1ks5L+U7UNIuK3wIkVyjcBZ1TZZj4wv0J5K9DbeEe/cs/CzKy7vp6G2p2mrc4GvhYRXwNG1a9ajeXZUGZm3fW1Z7FF0jzgY8Bfp1NLTfWrVmN5NpSZWXd97Vl8iOyK7E9GxJ/IrnP4p7rVqsHcszAz665PYZEC4rvAYZLeC+yIiNvqWrMGKo5ZeIDbzAz6fruPi4BlZDOXLgIek/TBelaskXydhZlZd30ds/g7smssNkB2DQXwIHB3vSrWSL43lJlZd30ds8h1BkWyaS+2PeT4+yzMzLrra8/ix5J+AtyZXn8IuL8+VWo8f5+FmVl3fQqLiLhS0gXAqWQ39rspIu6ta80ayD0LM7Pu+tqzICLuAe6pY10OGnnPhjIz66bXsJC0hcq3AxfZ9xuNrkutGsw9CzOz7noNi4gYsLf06E3e94YyM+tmwM5o2h+dt/vwdRZmZhmHRQV5X2dhZtaNw6ICj1mYmXXnsKjAs6HMzLpzWFSQl3sWZmalHBYV5HIiJ49ZmJl1clhUUcjl2OXZUGZmgMOiqnxOHrMwM0scFlUUcvKYhZlZ4rCoIp+XxyzMzBKHRRXuWZiZFTksqsjnRIcHuM3MAIdFVYVczj0LM7PEYVFFIe/ZUGZmneoWFpKOlvSQpGckPS3p8lR+pKQHJK1Kz0eUbDNP0mpJz0k6q6T8ZElPpveul9Il1nWU95iFmVmXevYsdgOfj4hjgJnAZZKOBa4ClkbEVGBpek16bw5wHDALuEFSPu3rRmAuMDU9ZtWx3kA2wO3ZUGZmmbqFRUSsi4gn0vIW4BlgEjAbWJBWWwCcn5ZnA3dFRHtEPA+sBmZImgiMjohHIyKA20q2qZu8xyzMzLockDELSZOBE4HHgAkRsQ6yQAHGp9UmAWtKNmtLZZPScnl5pePMldQqqXXjxo37VWf3LMzMiuoeFpJGAvcAn4uI13pbtUJZ9FLeszDipoiYHhHTx40bt/eVLeExCzOzorqGhaQmsqD4bkT8IBWvT6eWSM8bUnkbcHTJ5i3A2lTeUqG8rgq+N5SZWZd6zoYScDPwTERcW/LWIuCStHwJcF9J+RxJQyVNIRvIXpZOVW2RNDPt8+Ml29RNPid/B7eZWVKo475PBT4GPClpRSr7EnANsFDSpcCLwIUAEfG0pIXASrKZVJdFREfa7jPArcAwYHF61FUhL9p3uWdhZgZ1DIuI+CWVxxsAzqiyzXxgfoXyVuD4/qtdbdlsqI7aK5qZDQK+grsKz4YyMytyWFTh2VBmZkUOiyo8G8rMrMhhUYV7FmZmRQ6LKjxmYWZW5LCoIp/L+ToLM7PEYVGFexZmZkUOiyryeY9ZmJl1clhU4dlQZmZFDosqPBvKzKzIYVGFxyzMzIocFlX4m/LMzIocFlW4Z2FmVuSwqCKfwiL72m8zs8HNYVFFIZfdXd2noszMHBZV5fNZWPhUlJmZw6Iq9yzMzIocFlXkc9mPpsP3hzIzc1hUU+xZ+CpuMzOHRRX5nMcszMw6OSyqaMp7zMLMrJPDooquMQuHhZmZw6Iaz4YyMytyWFRRHLPwALeZmcOiCvcszMyK6hYWkm6RtEHSUyVlR0p6QNKq9HxEyXvzJK2W9Jyks0rKT5b0ZHrvekmqV51LdfYs/D3cZmb17VncCswqK7sKWBoRU4Gl6TWSjgXmAMelbW6QlE/b3AjMBaamR/k+66Lg232YmXWpW1hExM+Bl8uKZwML0vIC4PyS8rsioj0ingdWAzMkTQRGR8Sjkd3+9baSbeqqczaUT0OZmR34MYsJEbEOID2PT+WTgDUl67Wlsklpuby8IklzJbVKat24ceN+VbTgi/LMzLocLAPclcYhopfyiiLipoiYHhHTx40bt18Vyvt2H2ZmXQ50WKxPp5ZIzxtSeRtwdMl6LcDaVN5Sobzu3LMwMys60GGxCLgkLV8C3FdSPkfSUElTyAayl6VTVVskzUyzoD5esk1d5T111sysS6FeO5Z0J3A6MFZSG/BV4BpgoaRLgReBCwEi4mlJC4GVwG7gsojoSLv6DNnMqmHA4vSou4JvUW5m1qVuYRERH67y1hlV1p8PzK9Q3goc349V6xP3LMzMig6WAe6Djq+zMDMrclhU4dlQZmZFDosqPBvKzKzIYVGFxyzMzIocFlUU/OVHZmZdHBZVuGdhZlbksKiia8yiwwPcZmYOiyryefcszMw6OSyq8GwoM7Mih0UVHrMwMytyWFTRORvKX6tqZuawqCp1LOjwFdxmZg6LaiRRyMmnoczMcFj0Kp+TB7jNzHBY9Mo9CzOzjMOiF+5ZmJllHBa9KORzvkW5mRkOi14V3LMwMwMcFr0q5OTrLMzMcFj0Kp93z8LMDBwWvSrkcp4NZWaGw6JXng1lZpZxWPQiu87Cs6HMzBwWvXDPwsws47Ao1761a9FXcJuZZQqNrsBB5/YPgHLwjk/TpCPZvrODiEBSo2tmZtYwh0zPQtIsSc9JWi3pqrocZM8eOHY2vLYWvn8J33r5Uv5mzQ3Mu+afuOFHj/HEi6/w8radRLi3YWaDiw6FDz5JeeB3wHuANuBx4MMRsbLaNtOnT4/W1tZ9O+CeDvjdj9n96I3kXnyEXHQAsCEO57UYzjaNYHdhOB25oUR+CHvyQ9mTH5q9LjRDoZkoNKOmZtQ0DA0ZTm7IcHJDhlEYOpz80OEUmpoZMrSZwpChDGkewZDhoxgybBTNQ4cwtJDv+qY+M7MDSdLyiJheXn6onIaaAayOiD8ASLoLmA1UDYv9ksvDW86l8JZzYec2WPtrtq5+hJ0v/Y7cts2M2L6Z3M5t5PZsI797J4WdO2mKnQyJnQxhJ83s3OdDd4TYSRO7KNBBnj3KsYccexCgrueQCGoHSnEdVSirtm6ZsmKV/YERJafoyt+rtl5vqu2jdPu+tL3ivomKx6hWt/05Ti4CCBAEOaLP++tcZ2/+kOuPPy56O15f6tRff+DU4xjFfXb+5ij9i5SW9vw3qnW8anU9EH/sdW9T6VHHf6GVoc3D+/Voh0pYTALWlLxuA95RvpKkucBcgDe84Q39c+QhI2DyaYycfBoj+7pNBOxup2Pndtq3b6V9+1Z2bt/GzvbX2b3jdXbvfJ3dO9vZvaudjl3txM5tRPs22PU6sbud2L0TOtqzHk7ngz3ZfiM9Q1YGFf+/BsX/QD3qlnR/vw8fTBGQPlQ7f6FElJVnpZVrVKSIih/QxfLy96LbOvujeNziB2Cl+nQeR0TND/nyn/Ue5bo+kojOpdrTsMv305dwqfjvvI8qHa8vderPOtTrGKX7LI2LbN+dpdV+P/q23/6oZ1+V/66V/mE4Qf0/wnCohEXtTx8gIm4CboLsNFS9K1WVBE3N5JuaGT7iCPo3383MDrxDZYC7DTi65HULsLZBdTEzG3QOlbB4HJgqaYqkIcAcYFGD62RmNmgcEqehImK3pP8K/ATIA7dExNMNrpaZ2aBxSIQFQETcD9zf6HqYmQ1Gh8ppKDMzayCHhZmZ1eSwMDOzmhwWZmZW0yFxb6h9IWkj8Md93Hws8Od+rM6hYDC2GQZnuwdjm2Fwtntf2vzGiBhXXjhgw2J/SGqtdCOtgWwwthkGZ7sHY5thcLa7P9vs01BmZlaTw8LMzGpyWFR2U6Mr0ACDsc0wONs9GNsMg7Pd/dZmj1mYmVlN7lmYmVlNDgszM6vJYVFC0ixJz0laLemqRtenXiQdLekhSc9IelrS5an8SEkPSFqVno9odF37m6S8pF9L+mF6PRjafLikuyU9m/7NTxno7Zb039L/7ack3SmpeSC2WdItkjZIeqqkrGo7Jc1Ln2/PSTprb47lsEgk5YFvAGcDxwIflnRsY2tVN7uBz0fEMcBM4LLU1quApRExFViaXg80lwPPlLweDG3+GvDjiHgLcAJZ+wdsuyVNAj4LTI+I48m+1mAOA7PNtwKzysoqtjP9js8Bjkvb3JA+9/rEYVE0A1gdEX+IiJ3AXcDsBtepLiJiXUQ8kZa3kH14TCJr74K02gLg/IZUsE4ktQDnAt8pKR7obR4N/A1wM0BE7IyIzQzwdpN9/cIwSQVgONk3aw64NkfEz4GXy4qrtXM2cFdEtEfE88Bqss+9PnFYFE0C1pS8bktlA5qkycCJwGPAhIhYB1mgAOMbWLV6uA74ArCnpGygt/kvgI3Av6bTb9+RNIIB3O6IeAn4Z+BFYB3wakQsYQC3uUy1du7XZ5zDokgVygb0vGJJI4F7gM9FxGuNrk89SXovsCEilje6LgdYATgJuDEiTgS2MTBOv1SVztHPBqYARwEjJF3c2FodFPbrM85hUdQGHF3yuoWs6zogSWoiC4rvRsQPUvF6SRPT+xOBDY2qXx2cCpwn6QWyU4zvknQ7A7vNkP2/bouIx9Lru8nCYyC3+93A8xGxMSJ2AT8A/oqB3eZS1dq5X59xDouix4GpkqZIGkI2ELSowXWqC0kiO4f9TERcW/LWIuCStHwJcN+Brlu9RMS8iGiJiMlk/7Y/jYiLGcBtBoiIPwFrJL05FZ0BrGRgt/tFYKak4en/+hlk43IDuc2lqrVzETBH0lBJU4CpwLK+7tRXcJeQdA7Zee08cEtEzG9sjepD0mnAL4AnKZ6//xLZuMVC4A1kv3AXRkT54NkhT9LpwBUR8V5JYxjgbZY0jWxQfwjwB+ATZH8oDth2S7oa+BDZzL9fA/8FGMkAa7OkO4HTyW5Fvh74KvBvVGmnpL8DPkn2c/lcRCzu87EcFmZmVotPQ5mZWU0OCzMzq8lhYWZmNTkszMysJoeFmZnV5LAwO8hIOr3zrrhmBwuHhZmZ1eSwMNtHki6WtEzSCknfSt+VsVXS/5b0hKSlksaldadJ+pWk30q6t/M7BiT9R0kPSvpN2uYv0+5HlnwHxXfTlchmDeOwMNsHko4hu0L41IiYBnQAHwVGAE9ExEnAz8iuqAW4DfhiRLyN7Mr5zvLvAt+IiBPI7l+0LpWfCHyO7LtV/oLs3lZmDVNodAXMDlFnACcDj6c/+oeR3bBtD/C9tM7twA8kHQYcHhE/S+ULgO9LGgVMioh7ASJiB0Da37KIaEuvVwCTgV/WvVVmVTgszPaNgAURMa9bofTlsvV6u59Ob6eW2kuWO/DvqjWYT0OZ7ZulwAcljYeu7z1+I9nv1AfTOh8BfhkRrwKvSPrrVP4x4GfpO0TaJJ2f9jFU0vAD2QizvvJfK2b7ICJWSvrvwBJJOWAXcBnZlwsdJ2k58CrZuAZkt4r+ZgqDzju/QhYc35L092kfFx7AZpj1me86a9aPJG2NiJGNrodZf/NpKDMzq8k9CzMzq8k9CzMzq8lhYWZmNTkszMysJoeFmZnV5LAwM7Oa/j+uHGnhTdW5VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(hist_1.history['loss'])\n",
    "plt.plot(hist_1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
